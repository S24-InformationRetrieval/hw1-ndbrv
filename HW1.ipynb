{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4fbf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff22e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, ps):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5fd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_map = {}\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def parse_file(file_path):\n",
    "    with open(file_path, 'r') as file_object:\n",
    "        current_docno = None\n",
    "        current_text = \"\"\n",
    "        text_body = False\n",
    "        for line in file_object:\n",
    "            docno_match = re.search(r'<DOCNO>(.*?)</DOCNO>', line)\n",
    "            if docno_match:\n",
    "                current_docno = docno_match.group(1).strip()\n",
    "\n",
    "            # Find TEXT\n",
    "            text_match_start = re.search(r'<TEXT>', line)\n",
    "            text_match_end = re.search(r'</TEXT>', line)\n",
    "            if text_match_start:\n",
    "                text_body = True\n",
    "                continue\n",
    "            elif text_match_end:\n",
    "                text_body = False\n",
    "                \n",
    "            if text_body:\n",
    "                current_text+= line.strip()\n",
    "            \n",
    "            # Check if both DOCNO and TEXT are found\n",
    "            if current_docno and current_text != \"\" and text_body == False:\n",
    "                text_map[current_docno] = stem_text(current_text,ps)\n",
    "                # Reset for the next document\n",
    "                current_docno = None\n",
    "                current_text = \"\"\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188cd6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('AP_DATA/ap89_collection'):\n",
    "    file_path = os.path.join('AP_DATA/ap89_collection', filename)\n",
    "    parse_file(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebd3817e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84676"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a33be783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP890626-0001\n"
     ]
    }
   ],
   "source": [
    "for i in text_map.keys():\n",
    "    if vector_map[i] ==  {}:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04bbca1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AP890101-0001'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docnos = list(text_map.keys())\n",
    "docnos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78e549df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_path = 'config/stoplist.txt'\n",
    "\n",
    "with open(sw_path) as file:\n",
    "    stopwords = [line.strip() for line in file]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3be92eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15b933ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def process_content(text):\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "\n",
    "    filtered_words = [word for word in filtered_words if (word not in string.punctuation and word != \"``\")]\n",
    "\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32ec66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in zip(text_map.keys(), text_map.values()):\n",
    "    text_map[key] = process_content(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34ae8e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84676"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d1f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5846fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3856fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"ap89_data\"\n",
    "\n",
    "configurations = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords_path\": \"my_stoplist.txt\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0ddaa19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ap89_data'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=index_name, body=configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e28673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(_id, text):\n",
    "    es.index(index=index_name, body={'content': text}, id=_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in text_map:\n",
    "    add_data(key, text_map[key])\n",
    "    \n",
    "print(\"All documents have been added to the index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82793afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_query = {'85': 'alleg corrupt public offici government jurisdict',\n",
    "                '59': 'weather caus fatal',\n",
    "                '56' : 'prime lend rate',\n",
    "                '71': 'prime lend rate',\n",
    "                '64': 'hostage',\n",
    "                '62': \"militari coup d'etat\",\n",
    "                '93': 'nation rifl associ nra',\n",
    "                '99': 'iran contra',\n",
    "                '58': 'rail strike',\n",
    "                '77': 'poach wildlif',\n",
    "                '54': 'contract agreement reserv launch commerci satellit',\n",
    "                '87': 'current crimin action offic fail u.s financi institut',\n",
    "                '94': 'crime comput',\n",
    "                '100': 'communist industri state regul transfer high tech good technolog',\n",
    "                '89': 'invest opec member state downstream oper',\n",
    "                '61': 'israel iran contra',\n",
    "                '95': 'comput crime solv',\n",
    "                '68': 'studi concern safeti manufactur employe instal worker fine diamet fiber insul',\n",
    "                '57': 'mci bell',\n",
    "                '97': 'instanc fiber optic technolog',\n",
    "                '98': 'fiber optic equip',\n",
    "                '60': 'controversi standard perform determin salari level incent pay contrast determin basi senior longev job',\n",
    "                '80': '1988 presidenti',\n",
    "                '63': 'machin translat',\n",
    "                '91': 'acquisit weapon'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c57675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manual_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5888c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_search(query):\n",
    "    \n",
    "    search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content\": query\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    res_es_search = es.search(index='ap89_data1', body=search_query, size=1000)\n",
    "    return res_es_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c784f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in manual_query.keys():\n",
    "    res = ES_search(manual_query[query])['hits']['hits'][:1000]\n",
    "    with open('query_result_es_builtin.txt','a') as f:\n",
    "        for i,hit in enumerate(res):  \n",
    "            res_string = query + \" \" + 'Q0' + \" \" + hit['_id'] + \" \" + str(i+1) + \" \" + str(hit['_score']) + \" \" + \"Exp\" + '\\n'\n",
    "            f.write(res_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "559d6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_map = {}\n",
    "\n",
    "def get_term_vectors(doc_id):\n",
    "    term_vector_request = {\n",
    "        \"index\": \"ap89_data1\",\n",
    "        \"id\" : doc_id,\n",
    "        \"doc_type\": \"_doc\",\n",
    "        \"fields\": [\"content\"],\n",
    "        \"term_statistics\": True}\n",
    "    vector_map[doc_id] = es.termvectors(**term_vector_request)['term_vectors']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b329e858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\elasticsearch\\connection\\base.py:208: ElasticsearchWarning: [types removal] Specifying types in term vector requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "for doc in text_map.keys():\n",
    "    get_term_vectors(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "879b7e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84676"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbeb4069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"celluloid torch ha pass newgener filmmak grew 1960s platoon run empti 1969 mississippiburn movi releas past two fromwrit director brought experi thatturbul decad screen contemporari '60 filmmak ofth '80 's natur robert friedman senior vicepresid worldwid advertis public warner bros.chri gerolmo wrote screenplay mississippiburn note sheer passag time ha allow andoth express feel decade distanc import believ 's lot ofthink time america gener vietnam war wa defin experi mani peopl the'60 shatter consensu unit state right moral duti interven conflict world eventoday politician talk disparagingli vietnam syndrom inref countri 's reluct militari forc tosettl disputes think futur historian talk vietnam thenear destruct american societi uri brofenbrenn aprofessor sociolog cornel university world war ii knew fight invietnam full metal jacket garden stone platoon goodmorn vietnam hamburg hill bat 21 thewar dramat backdrop show shape charact lives.th vietnam war ha remain emot issu unitedst veteran struggl come term theirexperi wa oliv stone wrote direct theacademi award-win platoon platoon eight time john j. anderson palmbeach counti sheriff 's lieuten serv vietnam 1966-67 cri time ... third fourthtim platoon help understand stone base platoon hi experi agrunt film brought issu resolved peopl respond fact 's real they'recuri war vietnam 20 said.whil southeast asia wa pivot foreign issu americansocieti '60 civil right wa major domest issu thecivil right movement reach peak freedom summer of1964 larg group volunt head south help registerblack voters.in five corner movi summer '64 bronxstar jodi foster friend play tim robbin leav hisneighborhood volunt south rev martinluth king jr. television.alan parker 's mississippi burn focus incid thatcloud mississippi summer project 1,000 young volunteersfrom mainstream america swept state help regist blackvot movi fiction account disappearanceand slay three civil right worker michael schwerner andrewgoodman jame chaney.they report miss june 21 sever hour beingstop speed near philadelphia miss nationallypublic search bodi discov aug. 4 farmjust outsid town.on recal incid wa gerolmo student inth new york public school system time screenwrit saidth incid power effect hi way thinking wa time consid countri couldb wrong gerolmo said.th film star willem dafo gene hackman star fbi agentswho tri find bodi miss worker overcomefierc local resist solv crime.in offbeat outrag way john water hairspray discuss integr baltimor 1963 group ofteen-ag tri break barrier segreg danceshow.also set baltimor barri levinson 's tin men starringdanni devito richard dreyfuss two slick aluminum sidingsalesmen earli '60 movi mirror squarelymiddle-class cultur wa caught sex politicsand drugs.instead focus well-known histor event writer-director ernest thompson take person approach 1969 robert downey jr. keifer sutherland star collegestud battl parent sex drug andth vietnam war wa 19 1969 wa fulcrum time saidthompson wa student american univers time ithink wa right time growth artist aman tri write someth happen youth run empti take place '80 '60 aremuch evid judd hirsch christin lahti play anti-waractivist sabatog napalm plant 1970 forc toliv underground two children.naomi foner wrote run empti serv asth film 's execut produc grew brooklyn n.y. thedaught sociologist experi made foner wellqualifi give run empti strong polit theme live time 've find right wayto present thi gener foner member therad student democrat societi attend graduateschool columbia university.fon taught harlem 's head start program helpedregist voter south carolina mani young peopl arecuri happen '60s lot think wa excit time weresorri miss said.brofenbrenn movi good indic concern ofth gener public principl impact media theyreflect valu larger society film veri power art medium believ itveri accur reflect onli prevail comingtrend 's becaus film writer writer arepercept peopl messag 's\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_map['AP890101-0001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0826a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_doc_len():\n",
    "    total_words = vector_map['AP890306-0069']['content']['field_statistics']['sum_ttf']\n",
    "    return total_words / 84675    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0deb1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_len = get_avg_doc_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "46da4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_term_freqs = {}\n",
    "for doc in text_map.keys():\n",
    "    query_term_freqs[doc] = {}\n",
    "    for query in manual_query.keys():\n",
    "        query_term_freqs[doc][query] = []\n",
    "        query_term_dfw[doc][query] = []\n",
    "        for word in manual_query[query].split():\n",
    "            query_term_freqs[doc][query].append(get_term_freq(word,doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c2fc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_freq(term, doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 0\n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    ##print(terms)\n",
    "    if term in terms.keys():\n",
    "        return terms[term]['term_freq']\n",
    "    else:\n",
    "        return 0\n",
    "def get_doc_len(doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 0\n",
    "    doc_terms = vector_map[doc]['content']['terms']\n",
    "    doc_len = 0\n",
    "    for t in doc_terms.keys():\n",
    "        doc_len+= doc_terms[t]['term_freq']\n",
    "    return doc_len\n",
    "\n",
    "def get_dfw(term, doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 1\n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    if term in terms.keys():\n",
    "        return terms[term]['term_freq']\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_vocab_size():\n",
    "    vocab = []\n",
    "    for doc in text_map.keys():\n",
    "        doc_terms = vector_map[doc]['content']['terms']\n",
    "        for term in doc_terms.keys():\n",
    "            if term not in vocab:\n",
    "                vocab.append(term)\n",
    "                \n",
    "    return len(vocab)\n",
    "\n",
    "def get_cfw(term, doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 1\n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    if term in terms.keys():\n",
    "        return terms[term]['ttf']\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b19b59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_request = {\n",
    "        \"aggs\": {\n",
    "            \"vocabulary_size\": {\n",
    "                \"cardinality\": {\n",
    "                    \"field\": 'content'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Perform the search request with the aggregation\n",
    "search_request = {\n",
    "       \"query\": {\n",
    "         \"match_all\": {}\n",
    "        },\n",
    "        \"size\": 0,\n",
    "        **aggregation_request\n",
    "    }\n",
    "\n",
    "    # Execute the search request\n",
    "search_results = es.search(index='ap89_data1', body=search_request)\n",
    "\n",
    "    # Extract the vocabulary size from the aggregation response\n",
    "vocabulary_size = search_results['aggregations']['vocabulary_size']['value']\n",
    "    \n",
    "V = vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "712e8074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1404886\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fbfb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_tf(query):\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = tf_wd / (tf_wd + 0.5 + 1.5*(doc_len / avg_doc_len))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "788291ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(query):\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = (tf_wd / (tf_wd + 0.5 + 1.5*(doc_len / avg_doc_len))) * math.log(84676/get_dfw(word,doc))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a025890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_bm25(query):\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    k2 = 100\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            tf_wq = query_list.count(word)\n",
    "            first_term = math.log((84676 + 0.5) / (get_dfw(word,doc)+0.5))\n",
    "           \n",
    "            second_term = (tf_wd + k1*tf_wd) / (tf_wd + k1*((1-b) + b*doc_len/avg_doc_len))\n",
    "           \n",
    "            third_term = (tf_wq + k2*tf_wq) / (tf_wq + k2)\n",
    "            \n",
    "            score = first_term * second_term * third_term\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5692b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_laplace(query):\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = math.log((tf_wd+1) / (doc_len + V))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "510f7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_jm(query):\n",
    "    l = 0.5\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        query_list = manual_query[query].split() \n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            if doc_len == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = l*(tf_wd/doc_len) + (1-l)*(get_cfw(word,doc) / (V))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "24f03ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.455452538256885\n"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "for doc in text_map.keys():\n",
    "    ##print(okapi_tf(manual_query['85'], doc))\n",
    "    res_list.append(okapi_tf(manual_query['85'], doc))\n",
    "res_list.sort(reverse=True)\n",
    "print(res_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1adcd03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_res(result):\n",
    "    for rank, i  in enumerate(result['hits']['hits']):\n",
    "        score = i['_score']\n",
    "        doc_id = i['_id']\n",
    "        print(score, doc_id, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b20d8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(model, query, filename):\n",
    "    ##query_string = manual_query[query]\n",
    "    results = model(query)\n",
    "    #for doc in text_map.keys():\n",
    "        #model_out = model(query,doc)\n",
    "        #if model_out[1] > 0:\n",
    "            #results.append(model_out)\n",
    "            \n",
    "            \n",
    "    results.sort(key=lambda a: a[1], reverse=True)\n",
    "    results = results[:1000]\n",
    "    \n",
    "    with open(filename,'a') as f:\n",
    "        for i in range(len(results)):  \n",
    "            res_string = query + \" \" + 'Q0' + \" \" + results[i][0] + \" \" + str(i+1) + \" \" + str(results[i][1]) + \" \" + \"Exp\" + '\\n'\n",
    "            f.write(res_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a13d2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##OkapiTF\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(okapi_tf, query_num, 'query_result_okapitf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0003ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##TFIDF\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(tf_idf, query_num, 'query_result_tfidf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "821d2838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##BM25\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(okapi_bm25, query_num, 'query_result_bm25.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f7edcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##LM Laplace\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(lm_laplace, query_num, 'query_result_lmlaplace.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "365142d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##LM JM\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(lm_jm, query_num, 'query_result_lmjm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a6f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
