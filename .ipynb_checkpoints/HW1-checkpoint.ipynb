{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fbf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ff22e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, ps):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "fe5fd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_map = {}\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def parse_file(file_path):\n",
    "    with open(file_path, 'r') as file_object:\n",
    "        current_docno = None\n",
    "        current_text = \"\"\n",
    "        text_body = False\n",
    "        for line in file_object:\n",
    "            docno_match = re.search(r'<DOCNO>(.*?)</DOCNO>', line)\n",
    "            if docno_match:\n",
    "                current_docno = docno_match.group(1).strip()\n",
    "\n",
    "            # Find TEXT\n",
    "            text_match_start = re.search(r'<TEXT>', line)\n",
    "            text_match_end = re.search(r'</TEXT>', line)\n",
    "            if text_match_start:\n",
    "                text_body = True\n",
    "                continue\n",
    "            elif text_match_end:\n",
    "                text_body = False\n",
    "                \n",
    "            if text_body:\n",
    "                current_text+= line.strip()\n",
    "            \n",
    "            # Check if both DOCNO and TEXT are found\n",
    "            if re.search(r'</DOC>', line):\n",
    "                text_map[current_docno] = stem_text(current_text,ps)\n",
    "                # Reset for the next document\n",
    "                current_text = \"\"\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "188cd6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('AP_DATA/ap89_collection'):\n",
    "    file_path = os.path.join('AP_DATA/ap89_collection', filename)\n",
    "    parse_file(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "ebd3817e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84678"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "78e549df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_path = 'stoplist.txt'\n",
    "\n",
    "with open(sw_path) as file:\n",
    "    stopwords = [line.strip() for line in file]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3be92eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "15b933ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def process_content(text):\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "\n",
    "    filtered_words = [word for word in filtered_words if (word not in string.punctuation)]\n",
    "    \n",
    "    clean_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "32ec66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in zip(text_map.keys(), text_map.values()):\n",
    "    text_map[key] = process_content(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "34ae8e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84678"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35d1f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch7 import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b88abf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch7\n",
      "  Downloading elasticsearch7-7.17.9-py2.py3-none-any.whl (386 kB)\n",
      "     ---------------------------------------- 0.0/386.4 kB ? eta -:--:--\n",
      "     ----------- -------------------------- 112.6/386.4 kB 2.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 368.6/386.4 kB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 386.4/386.4 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\nikola\\anaconda3\\lib\\site-packages (from elasticsearch7) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in c:\\users\\nikola\\anaconda3\\lib\\site-packages (from elasticsearch7) (1.26.15)\n",
      "Installing collected packages: elasticsearch7\n",
      "Successfully installed elasticsearch7-7.17.9\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5846fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "3856fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"ap89_data_final\"\n",
    "\n",
    "configurations = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": stopwords\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a0ddaa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\AppData\\Local\\Temp\\ipykernel_17272\\3868789886.py:1: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(index=index_name, body=configurations)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ap89_data'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=index_name, body=configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e28673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(_id, text):\n",
    "    es.index(index=index_name, body={'content': text}, id=_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3d5e16fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\AppData\\Local\\Temp\\ipykernel_17272\\999600255.py:2: DeprecationWarning: The 'body' parameter is deprecated for the 'index' API and will be removed in a future version. Instead use the 'document' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.index(index=index_name, body={'content': text}, id=_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been added to the index\n"
     ]
    }
   ],
   "source": [
    "for key in text_map:\n",
    "    add_data(key, text_map[key])\n",
    "    \n",
    "print(\"All documents have been added to the index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "e744c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 corrupt offici govern\n",
      "59 weather dead\n",
      "56 prime lend rate\n",
      "71 incurs border militari guerilla\n",
      "64 hostag\n",
      "62 militari coup\n",
      "93 nation rifl associ nra\n",
      "99 develop iran contra\n",
      "58 rail strike\n",
      "77 poach\n",
      "54 commerci launch satellit\n",
      "87 crimin current action bank fail offic\n",
      "94 comput crime illeg\n",
      "100 high-tech dual-us regul transfer technolog\n",
      "89 invest opec downstream oper\n",
      "61 israel iran contra\n",
      "95 comput crime solv law\n",
      "68 safeti instal employ fine-diamet insul\n",
      "57 mci bell\n",
      "97 fiber optic\n",
      "98 fiber optic equip\n",
      "60 pay senior\n",
      "80 presidenti bush dukaki\n",
      "63 translat\n",
      "91 weapon armi\n"
     ]
    }
   ],
   "source": [
    "manual_query = {'85': 'corrupt offici government',\n",
    "                '59': 'weather dead',\n",
    "                '56' : 'prime lend rate',\n",
    "                '71': 'incursion border military guerilla',\n",
    "                '64': 'hostage',\n",
    "                '62': \"militari coup d'etat\",\n",
    "                '93': 'nation rifl associ nra',\n",
    "                '99': 'development iran contra',\n",
    "                '58': 'rail strike',\n",
    "                '77': 'poach',\n",
    "                '54': 'commerci launch satellit',\n",
    "                '87': 'criminal current action bank failed officer',\n",
    "                '94': 'computer crime illegal',\n",
    "                '100': 'high-tech dual-use regulate transfer technolog',\n",
    "                '89': 'invest opec member downstream oper',\n",
    "                '61': 'israel iran contra',\n",
    "                '95': 'computer crime solv law',\n",
    "                '68': 'safeti installation employe fine-diamet insulation',\n",
    "                '57': 'mci bell',\n",
    "                '97': 'fiber optic',\n",
    "                '98': 'fiber optic equip',\n",
    "                '60': 'pay senior',\n",
    "                '80': 'presidential bush dukakis',\n",
    "                '63': 'translation',\n",
    "                '91': 'weapon us army'\n",
    "}\n",
    "\n",
    "for q in manual_query.keys():\n",
    "    manual_query[q] = process_content(stem_text(manual_query[q],ps))\n",
    "    print(q,manual_query[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "5888c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_search(query_num):\n",
    "    \n",
    "    query_string = manual_query[query_num]\n",
    "    print(query_num, query_string)\n",
    "    \n",
    "#     search_query = {\n",
    "#     \"query\": {\n",
    "#         \"match\": {\n",
    "#             \"content\": query_string\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "    res_es_search = es.search(index='ap89_data_final', query={'match': {'content': query_string}}, size = 1000)\n",
    "    return res_es_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "c784f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 corrupt offici govern\n",
      "59 weather dead\n",
      "56 prime lend rate\n",
      "71 incurs border militari guerilla\n",
      "64 hostag\n",
      "62 militari coup\n",
      "93 nation rifl associ nra\n",
      "99 develop iran contra\n",
      "58 rail strike\n",
      "77 poach\n",
      "54 commerci launch satellit\n",
      "87 crimin current action bank fail offic\n",
      "94 comput crime illeg\n",
      "100 high-tech dual-us regul transfer technolog\n",
      "89 invest opec downstream oper\n",
      "61 israel iran contra\n",
      "95 comput crime solv law\n",
      "68 safeti instal employ fine-diamet insul\n",
      "57 mci bell\n",
      "97 fiber optic\n",
      "98 fiber optic equip\n",
      "60 pay senior\n",
      "80 presidenti bush dukaki\n",
      "63 translat\n",
      "91 weapon armi\n"
     ]
    }
   ],
   "source": [
    "##Get scores for ES built in\n",
    "with open('query_result_es_builtin.txt','w') as f:\n",
    "        f.write(\"\")\n",
    "for num in manual_query.keys():\n",
    "    res = ES_search(num)['hits']['hits']\n",
    "    with open('query_result_es_builtin.txt','a') as f:\n",
    "        for i,hit in enumerate(res):  \n",
    "            res_string = num + \" \" + 'Q0' + \" \" + hit['_id'] + \" \" + str(i+1) + \" \" + str(hit['_score']) + \" \" + \"Exp\" + '\\n'\n",
    "            f.write(res_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "559d6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_map = {}\n",
    "\n",
    "def get_term_vectors(doc_id):\n",
    "    term_vector_request = {\n",
    "        \"index\": \"ap89_data_final\",\n",
    "        \"id\" : doc_id,\n",
    "        \"doc_type\": \"_doc\",\n",
    "        \"fields\": [\"content\"],\n",
    "        \"term_statistics\": True}\n",
    "    vector_map[doc_id] = es.termvectors(**term_vector_request)['term_vectors']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "b329e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in text_map.keys():\n",
    "    get_term_vectors(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "879b7e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84678"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "0826a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_doc_len():\n",
    "    total_words = vector_map['AP890306-0069']['content']['field_statistics']['sum_ttf']\n",
    "    return total_words / 84678    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0deb1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_len = get_avg_doc_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "14f94028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251.69524551831645\n"
     ]
    }
   ],
   "source": [
    "print(avg_doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "3c2fc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_freq(term, doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 0\n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    \n",
    "    if term in terms.keys():\n",
    "        return terms[term]['term_freq']\n",
    "    else:\n",
    "        return 0\n",
    "def get_doc_len(doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 0\n",
    "    doc_terms = vector_map[doc]['content']['terms']\n",
    "    doc_len = 0\n",
    "    for t in doc_terms.keys():\n",
    "        doc_len+= doc_terms[t]['term_freq']\n",
    "    return doc_len\n",
    "\n",
    "def get_dfw(term):\n",
    "    \n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    \n",
    "    query_body = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content\": term  \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es.count(index=\"ap89_data_final\", body=query_body)\n",
    "    return response['count']\n",
    "\n",
    "\n",
    "def get_cfw(term, doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 0\n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    if term in terms.keys():\n",
    "        return terms[term]['ttf']\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "9a0564f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = set()\n",
    "\n",
    "\n",
    "for v_key in vector_map.keys():\n",
    "    if vector_map[v_key] == {}:\n",
    "        continue\n",
    "    vocab_size.update(set(vector_map[v_key]['content']['terms'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "219de9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "712e8074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1409243\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "1fbfb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_tf(query_key):\n",
    "    scores = []\n",
    "    query_list = manual_query[query_key].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = tf_wd / (tf_wd + 0.5 + 1.5*(doc_len / avg_doc_len))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "788291ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(query_key):\n",
    "    scores = []\n",
    "    query_list = manual_query[query_key].split()\n",
    "    dfw_list = [get_dfw(word) for word in query_list]\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word,dfw in zip(query_list,dfw_list):\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = (tf_wd / (tf_wd + 0.5 + 1.5*(doc_len / avg_doc_len))) * math.log(84678/dfw)\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "a025890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_bm25(query_key):\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    k2 = 500\n",
    "    scores = []\n",
    "    query_list = manual_query[query_key].split()\n",
    "    dfw_list = [get_dfw(word) for word in query_list]\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word, dfw in zip(query_list,dfw_list):\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            tf_wq = query_list.count(word)\n",
    "            first_term = math.log((84678 + 0.5) / (dfw+0.5))\n",
    "           \n",
    "            second_term = (tf_wd + k1*tf_wd) / (tf_wd + k1*((1-b) + b*doc_len/avg_doc_len))\n",
    "           \n",
    "            third_term = (tf_wq + k2*tf_wq) / (tf_wq + k2)\n",
    "            \n",
    "            score = first_term * second_term * third_term\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "d5692b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_laplace(query_key):\n",
    "    scores = []\n",
    "    query_list = manual_query[query_key].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = math.log((tf_wd+1) / (doc_len + V))\n",
    "            total_score+=score\n",
    "        if total_score != 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "510f7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_jm(query_key):\n",
    "    l = 0.7\n",
    "    scores = []\n",
    "    query_list = manual_query[query_key].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            if tf_wd == 0:\n",
    "                score = -10000\n",
    "                total_score += score\n",
    "            else:\n",
    "                score = (l*(tf_wd/doc_len)) + ((1-l)*(get_cfw(word,doc) / V))\n",
    "                total_score += math.log(score)\n",
    "        if total_score != 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b20d8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(model, query, filename):\n",
    "    ##query_string = manual_query[query]\n",
    "    results = model(query)\n",
    "    #for doc in text_map.keys():\n",
    "        #model_out = model(query,doc)\n",
    "        #if model_out[1] > 0:\n",
    "            #results.append(model_out)\n",
    "            \n",
    "            \n",
    "    results.sort(key=lambda a: a[1], reverse=True)\n",
    "    results = results[:1000]\n",
    "    \n",
    "    \n",
    "    with open(filename,'a') as f:\n",
    "        for i in range(len(results)):  \n",
    "            res_string = query + \" \" + 'Q0' + \" \" + results[i][0] + \" \" + str(i+1) + \" \" + str(results[i][1]) + \" \" + \"Exp\" + '\\n'\n",
    "            f.write(res_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "a13d2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##OkapiTF\n",
    "with open('query_result_okapitf.txt','w') as f:\n",
    "        f.write(\"\")\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(okapi_tf, query_num, 'query_result_okapitf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "c0003ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##TFIDF\n",
    "with open('query_result_tfidf.txt','w') as f:\n",
    "        f.write(\"\")\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(tf_idf, query_num, 'query_result_tfidf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "821d2838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##BM25\n",
    "with open('query_result_bm25.txt','w') as f:\n",
    "        f.write(\"\")\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(okapi_bm25, query_num, 'query_result_bm25.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "4f7edcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##LM Laplace\n",
    "with open('query_result_lmlaplace.txt','w') as f:\n",
    "        f.write(\"\")\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(lm_laplace, query_num, 'query_result_lmlaplace.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "365142d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  56\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##LM JM\n",
    "with open('query_result_lmjm.txt','w') as f:\n",
    "        f.write(\"\")\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(lm_jm, query_num, 'query_result_lmjm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75b142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
