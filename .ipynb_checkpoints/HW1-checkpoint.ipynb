{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff22e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, ps):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe5fd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_map = {}\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def parse_file(file_path):\n",
    "    with open(file_path, 'r') as file_object:\n",
    "        current_docno = None\n",
    "        current_text = \"\"\n",
    "        text_body = False\n",
    "        for line in file_object:\n",
    "            docno_match = re.search(r'<DOCNO>(.*?)</DOCNO>', line)\n",
    "            if docno_match:\n",
    "                current_docno = docno_match.group(1).strip()\n",
    "\n",
    "            # Find TEXT\n",
    "            text_match_start = re.search(r'<TEXT>', line)\n",
    "            text_match_end = re.search(r'</TEXT>', line)\n",
    "            if text_match_start:\n",
    "                text_body = True\n",
    "                continue\n",
    "            elif text_match_end:\n",
    "                text_body = False\n",
    "                \n",
    "            if text_body:\n",
    "                current_text+= line.strip()\n",
    "            \n",
    "            # Check if both DOCNO and TEXT are found\n",
    "            if current_docno and current_text != \"\" and text_body == False:\n",
    "                text_map[current_docno] = stem_text(current_text,ps)\n",
    "                # Reset for the next document\n",
    "                current_docno = None\n",
    "                current_text = \"\"\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "188cd6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('AP_DATA/ap89_collection'):\n",
    "    file_path = os.path.join('AP_DATA/ap89_collection', filename)\n",
    "    parse_file(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebd3817e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84676"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04bbca1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AP890101-0001'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docnos = list(text_map.keys())\n",
    "docnos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78e549df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_path = 'config/stoplist.txt'\n",
    "\n",
    "with open(sw_path) as file:\n",
    "    stopwords = [line.strip() for line in file]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3be92eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15b933ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def process_content(text):\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "\n",
    "    filtered_words = [word for word in filtered_words if word not in string.punctuation]\n",
    "\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32ec66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in zip(text_map.keys(), text_map.values()):\n",
    "    text_map[key] = process_content(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34ae8e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84678"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35d1f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5846fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3856fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"ap89_data1\"\n",
    "\n",
    "configurations = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords_path\": \"my_stoplist.txt\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0ddaa19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ap89_data1'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=index_name, body=configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e28673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(_id, text):\n",
    "    es.index(index=index_name, body={'content': text}, id=_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d5e16fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been added to the index\n"
     ]
    }
   ],
   "source": [
    "for key in text_map:\n",
    "    add_data(key, text_map[key])\n",
    "    \n",
    "print(\"All documents have been added to the index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82793afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_query = {'85': 'alleg corrupt public offici government jurisdict',\n",
    "                '59': 'weather caus fatal',\n",
    "                '71': 'prime lend rate',\n",
    "                '64': 'hostage',\n",
    "                '62': \"militari coup d'etat\",\n",
    "                '93': 'nation rifl associ nra',\n",
    "                '99': 'iran contra',\n",
    "                '58': 'rail strike',\n",
    "                '77': 'poach wildlif',\n",
    "                '54': 'contract agreement reserv launch commerci satellit',\n",
    "                '87': 'current crimin action offic fail u.s financi institut',\n",
    "                '94': 'crime comput',\n",
    "                '100': 'communist industri state regul transfer high tech good technolog',\n",
    "                '89': 'invest opec member state downstream oper',\n",
    "                '61': 'israel iran contra',\n",
    "                '95': 'comput crime solv',\n",
    "                '68': 'studi concern safeti manufactur employe instal worker fine diamet fiber insul',\n",
    "                '57': 'mci bell',\n",
    "                '97': 'instanc fiber optic technolog',\n",
    "                '98': 'fiber optic equip',\n",
    "                '60': 'controversi standard perform determin salari level incent pay contrast determin basi senior longev job',\n",
    "                '80': '1988 presidenti',\n",
    "                '63': 'machin translat',\n",
    "                '91': 'acquisit weapon'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5888c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES_search(query):\n",
    "    \n",
    "    search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content\": query\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    res_es_search = es.search(index='ap89_data1', body=search_query, size=1000)\n",
    "    return res_es_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c784f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in manual_query.keys():\n",
    "    res = ES_search(manual_query[query])['hits']['hits'][:1000]\n",
    "    with open('query_result_es_builtin.txt','a') as f:\n",
    "        for i,hit in enumerate(res):  \n",
    "            res_string = query + \" \" + 'Q0' + \" \" + hit['_id'] + \" \" + str(i) + \" \" + str(hit['_score']) + \" \" + \"Exp\" + '\\n'\n",
    "            f.write(res_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "559d6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_map = {}\n",
    "\n",
    "def get_term_vectors(doc_id):\n",
    "    term_vector_request = {\n",
    "        \"index\": \"ap89_data1\",\n",
    "        \"id\" : doc_id,\n",
    "        \"doc_type\": \"_doc\",\n",
    "        \"fields\": [\"content\"],\n",
    "        \"term_statistics\": True}\n",
    "    vector_map[doc_id] = es.termvectors(**term_vector_request)['term_vectors']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0826a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_doc_len():\n",
    "    total_words = vector_map['AP890306-0069']['content']['field_statistics']['sum_ttf']\n",
    "    return total_words / 84675    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0deb1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_len = get_avg_doc_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "46da4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_term_freqs = {}\n",
    "for doc in text_map.keys():\n",
    "    query_term_freqs[doc] = {}\n",
    "    for query in manual_query.keys():\n",
    "        query_term_freqs[doc][query] = []\n",
    "        query_term_dfw[doc][query] = []\n",
    "        for word in manual_query[query].split():\n",
    "            query_term_freqs[doc][query].append(get_term_freq(word,doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "3c2fc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_freq(term, doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 0\n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    ##print(terms)\n",
    "    if term in terms.keys():\n",
    "        return terms[term]['term_freq']\n",
    "    else:\n",
    "        return 0\n",
    "def get_doc_len(doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 0\n",
    "    doc_terms = vector_map[doc]['content']['terms']\n",
    "    doc_len = 0\n",
    "    for t in doc_terms.keys():\n",
    "        doc_len+= doc_terms[t]['term_freq']\n",
    "    return doc_len\n",
    "\n",
    "def get_dfw(term, doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 1\n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    if term in terms.keys():\n",
    "        return terms[term]['term_freq']\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_vocab_size():\n",
    "    vocab = []\n",
    "    for doc in text_map.keys():\n",
    "        doc_terms = vector_map[doc]['content']['terms']\n",
    "        for term in doc_terms.keys():\n",
    "            if term not in vocab:\n",
    "                vocab.append(term)\n",
    "                \n",
    "    return len(vocab)\n",
    "\n",
    "def get_cfw(term, doc):\n",
    "    if vector_map[doc] == {}:\n",
    "        return 1\n",
    "    terms = vector_map[doc]['content']['terms']\n",
    "    if term in terms.keys():\n",
    "        return terms[term]['ttf']\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b19b59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_request = {\n",
    "        \"aggs\": {\n",
    "            \"vocabulary_size\": {\n",
    "                \"cardinality\": {\n",
    "                    \"field\": 'content'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Perform the search request with the aggregation\n",
    "search_request = {\n",
    "       \"query\": {\n",
    "         \"match_all\": {}\n",
    "        },\n",
    "        \"size\": 0,\n",
    "        **aggregation_request\n",
    "    }\n",
    "\n",
    "    # Execute the search request\n",
    "search_results = es.search(index='ap89_data1', body=search_request)\n",
    "\n",
    "    # Extract the vocabulary size from the aggregation response\n",
    "vocabulary_size = search_results['aggregations']['vocabulary_size']['value']\n",
    "    \n",
    "V = vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "712e8074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1404886\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "1fbfb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_tf(query):\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = tf_wd / (tf_wd + 0.5 + 1.5*(doc_len / avg_doc_len))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "788291ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(query):\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = (tf_wd / (tf_wd + 0.5 + 1.5*(doc_len / avg_doc_len))) * math.log(84676/get_dfw(word,doc))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a025890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_bm25(query):\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    k2 = 100\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            tf_wq = query_list.count(word)\n",
    "            first_term = math.log((84676 + 0.5) / (get_dfw(word,doc)+0.5))\n",
    "           \n",
    "            second_term = (tf_wd + k1*tf_wd) / (tf_wd + k1*((1-b) + b*doc_len/avg_doc_len))\n",
    "           \n",
    "            third_term = (tf_wq + k2*tf_wq) / (tf_wq + k2)\n",
    "            \n",
    "            score = first_term * second_term * third_term\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d5692b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_laplace(query):\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        terms = query_term_freqs[doc][query] \n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = math.log((tf_wd+1) / (doc_len + V))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "510f7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_jm(query):\n",
    "    l = 0.5\n",
    "    scores = []\n",
    "    query_list = manual_query[query].split()\n",
    "    for doc in text_map.keys():\n",
    "        doc_len = get_doc_len(doc)\n",
    "        query_list = manual_query[query].split() \n",
    "        total_score = 0\n",
    "        for word in query_list:\n",
    "            tf_wd = get_term_freq(word,doc)\n",
    "            score = l*(tf_wd/doc_len) + (1-l)*(get_cfw(word,doc) / (V))\n",
    "            total_score+=score\n",
    "        if total_score > 0:    \n",
    "            scores.append((doc, total_score))\n",
    "    return scores\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "24f03ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.455452538256885\n"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "for doc in text_map.keys():\n",
    "    ##print(okapi_tf(manual_query['85'], doc))\n",
    "    res_list.append(okapi_tf(manual_query['85'], doc))\n",
    "res_list.sort(reverse=True)\n",
    "print(res_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1adcd03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_res(result):\n",
    "    for rank, i  in enumerate(result['hits']['hits']):\n",
    "        score = i['_score']\n",
    "        doc_id = i['_id']\n",
    "        print(score, doc_id, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b20d8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(model, query, filename):\n",
    "    ##query_string = manual_query[query]\n",
    "    results = model(query)\n",
    "    #for doc in text_map.keys():\n",
    "        #model_out = model(query,doc)\n",
    "        #if model_out[1] > 0:\n",
    "            #results.append(model_out)\n",
    "            \n",
    "            \n",
    "    results.sort(key=lambda a: a[1], reverse=True)\n",
    "    results = results[:1000]\n",
    "    \n",
    "    with open(filename,'a') as f:\n",
    "        for i in range(len(results)):  \n",
    "            res_string = query + \" \" + 'Q0' + \" \" + results[i][0] + \" \" + str(i) + \" \" + str(results[i][1]) + \" \" + \"Exp\" + '\\n'\n",
    "            f.write(res_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "a13d2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##OkapiTF\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(okapi_tf, query_num, 'query_result_okapitf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c0003ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##TFIDF\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(tf_idf, query_num, 'query_result_tfidf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "821d2838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##BM25\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(okapi_bm25, query_num, 'query_result_bm25.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "4f7edcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##LM Laplace\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(lm_laplace, query_num, 'query_result_lmlaplace.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "365142d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query num  85\n",
      "Query num  59\n",
      "Query num  71\n",
      "Query num  64\n",
      "Query num  62\n",
      "Query num  93\n",
      "Query num  99\n",
      "Query num  58\n",
      "Query num  77\n",
      "Query num  54\n",
      "Query num  87\n",
      "Query num  94\n",
      "Query num  100\n",
      "Query num  89\n",
      "Query num  61\n",
      "Query num  95\n",
      "Query num  68\n",
      "Query num  57\n",
      "Query num  97\n",
      "Query num  98\n",
      "Query num  60\n",
      "Query num  80\n",
      "Query num  63\n",
      "Query num  91\n"
     ]
    }
   ],
   "source": [
    "##LM JM\n",
    "for query_num in manual_query.keys():\n",
    "    print(\"Query num \", query_num)\n",
    "    process_model(lm_jm, query_num, 'query_result_lmjm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a6f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
